%NLP タスクを直接処理できる汎用アーキテクチャがあれば、どれほど素晴らしいだろうか。
%時代が進むにつれて、多くの人が自然とこのような考えを持つようになりました。そして、BERT は、この概念を実践した例の 1 つです。BERT 論文の著者たちは、Transformer %Encoder、大量のテキスト、2 つの事前学習目標を使用して、複数の NLP タスクに適用できる BERT モデルを事前にトレーニングしました。その後、これを基に、複数の下流タスクを %fine tune します。
%これが、最近 NLP 分野で非常に流行している 2 段階の転移学習です。
%1. LM Pretraining によって、自然言語をある程度「理解」できる汎用モデルを事前にトレーニングします。
%2. そのモデルを特徴抽出や下流の（教師あり）タスクの fine tune に使用します。
%具体的には、BERT は、Transformer Encoder を使用して、テキストの単語と単語の間の意味的な関係を学習します。また、Masked Language Modeling (MLM) と Next %Sentence Prediction (NSP) という 2 つの事前学習目標を使用して、テキストの文脈情報を考慮して、単語の意味をより正確に理解できるようにします。
%この BERT の技術は、自然言語処理のさまざまなタスクで大きな成果を上げており、テキストの要約、質問への回答、テキストの生成など、さまざまなアプリケーションに使用されています。

%Sample codes
%%bash
pip install transformers tqdm boto3 requests regex -q

