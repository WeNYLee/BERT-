タスク#1: Masked LM (MLM)
Masked Language Model (MLM) を提案しました。これは、任意の段落内で空白を埋めるクイズのアイデアに基づいており、
モデルは空白の前後の文脈を使用して、空白に適したテキストを予測します。モデルが同じ能力を達成できるようにするために、各トークンシーケンスからランダムに15%のトークンをマスク [MASK] し、
モデルに [MASK] に30,000語の中からどの語彙を入力するかを予測させました。

実際の実行では、最後の層のベクトル Encoder Output を Classification Layer に入力し、辞書と同じサイズの次元に変換します。
次に、Softmax を使用して、どの次元の語彙の確率値が最も高いかを確認し、その単語を予測単語として取得します。最後に、予測結果と正解答を比較して Cross Entropy Loss を計算し、モデルを更新します。

ただし、ここでは Pre-Train と Fine-Tune の不一致の問題が発生します。Fine-Tune では、Pre-Train のように [MASK] のようなものが出現しません。
この問題を解決するために、
作者は Noise の概念を導入しました。彼らは、先ほどランダムに選択した15%のトークンのうち、いくつかの操作を行いました。そのうちの80%は元の [MASK] のまま、10%はランダムなテキストに置き換え、
残りの10%は元々 [MASK] されていなかったトークンを維持しました。

